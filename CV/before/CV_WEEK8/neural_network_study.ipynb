{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "7bb547a71273723496801ba8d5c67702ef0837f2b218b3f175a648afb4e114e5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## neural_network_study.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-1.3882,  0.2852,  1.2394,  0.5345, -0.6625,  0.2691, -0.2566,  0.2842,\n",
       "           0.3016, -0.2141],\n",
       "         [-0.3795, -0.9243, -2.1069,  0.0254, -1.0579,  0.6431, -0.4024,  0.7362,\n",
       "          -0.3545,  0.5981],\n",
       "         [-1.7640, -0.3340, -0.6902,  0.0152,  0.4122,  1.4090, -0.7125, -0.4611,\n",
       "          -1.2843, -2.0920],\n",
       "         [ 0.4007, -0.1869,  0.6708,  0.7287, -0.6053,  0.9482, -0.8358,  1.3376,\n",
       "          -0.5377, -1.1441],\n",
       "         [-2.3333,  1.1642,  0.0454, -0.4109,  0.9035, -1.2355, -0.0835, -1.2553,\n",
       "          -0.9208, -0.4069],\n",
       "         [-0.5793, -0.0824, -1.7324,  0.8250,  0.9503, -1.5946,  0.6201, -1.5958,\n",
       "          -0.3042, -0.9528],\n",
       "         [-2.5500, -1.6139, -0.7529,  0.1285, -1.3814, -0.4257,  0.8806, -0.1410,\n",
       "           0.1853, -0.2760],\n",
       "         [ 0.7796,  0.6501,  0.6738, -2.3371, -0.6655, -1.1168,  0.6382, -1.6351,\n",
       "          -0.2606, -0.7088],\n",
       "         [-0.9767,  1.2501,  0.1272, -0.4378,  0.8377, -0.1693,  0.1710,  0.0587,\n",
       "          -0.4066,  1.1220],\n",
       "         [ 0.1456,  0.0841, -0.2054, -0.2771,  0.1110, -1.1210,  0.3015,  2.5579,\n",
       "           0.0528, -0.3700]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([ [1.], [1.], [0.], [1.], [0.], [1.], [0.], [1.], [1.], [0.] ])\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_in, n_h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_h, n_out),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "09809494\n",
      "epoch :  2039  loss :  0.05477767437696457\n",
      "epoch :  2040  loss :  0.05473340302705765\n",
      "epoch :  2041  loss :  0.054682422429323196\n",
      "epoch :  2042  loss :  0.05463724583387375\n",
      "epoch :  2043  loss :  0.0545874647796154\n",
      "epoch :  2044  loss :  0.05454229563474655\n",
      "epoch :  2045  loss :  0.05449158698320389\n",
      "epoch :  2046  loss :  0.05444863438606262\n",
      "epoch :  2047  loss :  0.05439681559801102\n",
      "epoch :  2048  loss :  0.054350823163986206\n",
      "epoch :  2049  loss :  0.054305028170347214\n",
      "epoch :  2050  loss :  0.054255761206150055\n",
      "epoch :  2051  loss :  0.05421070381999016\n",
      "epoch :  2052  loss :  0.0541611984372139\n",
      "epoch :  2053  loss :  0.05411679670214653\n",
      "epoch :  2054  loss :  0.05406716465950012\n",
      "epoch :  2055  loss :  0.05402389168739319\n",
      "epoch :  2056  loss :  0.053972989320755005\n",
      "epoch :  2057  loss :  0.053929902613162994\n",
      "epoch :  2058  loss :  0.053880125284194946\n",
      "epoch :  2059  loss :  0.053834639489650726\n",
      "epoch :  2060  loss :  0.05378810316324234\n",
      "epoch :  2061  loss :  0.05373981595039368\n",
      "epoch :  2062  loss :  0.05369459465146065\n",
      "epoch :  2063  loss :  0.053645920008420944\n",
      "epoch :  2064  loss :  0.053601838648319244\n",
      "epoch :  2065  loss :  0.05355280637741089\n",
      "epoch :  2066  loss :  0.053507573902606964\n",
      "epoch :  2067  loss :  0.053462546318769455\n",
      "epoch :  2068  loss :  0.053413860499858856\n",
      "epoch :  2069  loss :  0.053370267152786255\n",
      "epoch :  2070  loss :  0.053319383412599564\n",
      "epoch :  2071  loss :  0.0532788410782814\n",
      "epoch :  2072  loss :  0.053227148950099945\n",
      "epoch :  2073  loss :  0.05318561941385269\n",
      "epoch :  2074  loss :  0.05313597992062569\n",
      "epoch :  2075  loss :  0.05309011787176132\n",
      "epoch :  2076  loss :  0.053046803921461105\n",
      "epoch :  2077  loss :  0.052997417747974396\n",
      "epoch :  2078  loss :  0.05295558646321297\n",
      "epoch :  2079  loss :  0.05290531367063522\n",
      "epoch :  2080  loss :  0.052864037454128265\n",
      "epoch :  2081  loss :  0.05281324312090874\n",
      "epoch :  2082  loss :  0.052772365510463715\n",
      "epoch :  2083  loss :  0.05272331088781357\n",
      "epoch :  2084  loss :  0.052680451422929764\n",
      "epoch :  2085  loss :  0.05263303965330124\n",
      "epoch :  2086  loss :  0.05258907005190849\n",
      "epoch :  2087  loss :  0.052541524171829224\n",
      "epoch :  2088  loss :  0.05249815061688423\n",
      "epoch :  2089  loss :  0.05245095491409302\n",
      "epoch :  2090  loss :  0.052406370639801025\n",
      "epoch :  2091  loss :  0.05236157774925232\n",
      "epoch :  2092  loss :  0.05231481045484543\n",
      "epoch :  2093  loss :  0.052271634340286255\n",
      "epoch :  2094  loss :  0.05222528427839279\n",
      "epoch :  2095  loss :  0.05218012258410454\n",
      "epoch :  2096  loss :  0.05213528871536255\n",
      "epoch :  2097  loss :  0.05208989977836609\n",
      "epoch :  2098  loss :  0.05204569175839424\n",
      "epoch :  2099  loss :  0.051999133080244064\n",
      "epoch :  2100  loss :  0.0519561767578125\n",
      "epoch :  2101  loss :  0.05190763622522354\n",
      "epoch :  2102  loss :  0.051867593079805374\n",
      "epoch :  2103  loss :  0.05181824415922165\n",
      "epoch :  2104  loss :  0.051774851977825165\n",
      "epoch :  2105  loss :  0.0517326295375824\n",
      "epoch :  2106  loss :  0.05168475955724716\n",
      "epoch :  2107  loss :  0.051643528044223785\n",
      "epoch :  2108  loss :  0.051595114171504974\n",
      "epoch :  2109  loss :  0.051554955542087555\n",
      "epoch :  2110  loss :  0.05150597169995308\n",
      "epoch :  2111  loss :  0.05146624892950058\n",
      "epoch :  2112  loss :  0.051416587084531784\n",
      "epoch :  2113  loss :  0.051374685019254684\n",
      "epoch :  2114  loss :  0.051332008093595505\n",
      "epoch :  2115  loss :  0.051285095512866974\n",
      "epoch :  2116  loss :  0.05124397948384285\n",
      "epoch :  2117  loss :  0.05119717866182327\n",
      "epoch :  2118  loss :  0.05115474388003349\n",
      "epoch :  2119  loss :  0.05110964924097061\n",
      "epoch :  2120  loss :  0.05106734484434128\n",
      "epoch :  2121  loss :  0.05102100968360901\n",
      "epoch :  2122  loss :  0.050981443375349045\n",
      "epoch :  2123  loss :  0.05093342065811157\n",
      "epoch :  2124  loss :  0.050894416868686676\n",
      "epoch :  2125  loss :  0.05084659531712532\n",
      "epoch :  2126  loss :  0.05080680921673775\n",
      "epoch :  2127  loss :  0.05076029896736145\n",
      "epoch :  2128  loss :  0.050719015300273895\n",
      "epoch :  2129  loss :  0.05067399889230728\n",
      "epoch :  2130  loss :  0.05063123255968094\n",
      "epoch :  2131  loss :  0.05058864876627922\n",
      "epoch :  2132  loss :  0.05054344981908798\n",
      "epoch :  2133  loss :  0.05050305649638176\n",
      "epoch :  2134  loss :  0.050457410514354706\n",
      "epoch :  2135  loss :  0.050415389239788055\n",
      "epoch :  2136  loss :  0.05037252977490425\n",
      "epoch :  2137  loss :  0.05032847449183464\n",
      "epoch :  2138  loss :  0.0502866692841053\n",
      "epoch :  2139  loss :  0.050242651253938675\n",
      "epoch :  2140  loss :  0.050199706107378006\n",
      "epoch :  2141  loss :  0.05015694350004196\n",
      "epoch :  2142  loss :  0.05011393502354622\n",
      "epoch :  2143  loss :  0.050070904195308685\n",
      "epoch :  2144  loss :  0.050029050558805466\n",
      "epoch :  2145  loss :  0.04998447746038437\n",
      "epoch :  2146  loss :  0.04994422569870949\n",
      "epoch :  2147  loss :  0.04989927262067795\n",
      "epoch :  2148  loss :  0.04985819384455681\n",
      "epoch :  2149  loss :  0.049815576523542404\n",
      "epoch :  2150  loss :  0.049772825092077255\n",
      "epoch :  2151  loss :  0.04973141476511955\n",
      "epoch :  2152  loss :  0.04968647286295891\n",
      "epoch :  2153  loss :  0.04964776709675789\n",
      "epoch :  2154  loss :  0.049601368606090546\n",
      "epoch :  2155  loss :  0.04956292361021042\n",
      "epoch :  2156  loss :  0.04951713979244232\n",
      "epoch :  2157  loss :  0.049477867782115936\n",
      "epoch :  2158  loss :  0.04943416267633438\n",
      "epoch :  2159  loss :  0.049391988664865494\n",
      "epoch :  2160  loss :  0.04935341328382492\n",
      "epoch :  2161  loss :  0.04930967837572098\n",
      "epoch :  2162  loss :  0.049269817769527435\n",
      "epoch :  2163  loss :  0.04922683164477348\n",
      "epoch :  2164  loss :  0.049187175929546356\n",
      "epoch :  2165  loss :  0.04914359375834465\n",
      "epoch :  2166  loss :  0.049105558544397354\n",
      "epoch :  2167  loss :  0.04905982315540314\n",
      "epoch :  2168  loss :  0.0490243062376976\n",
      "epoch :  2169  loss :  0.0489775612950325\n",
      "epoch :  2170  loss :  0.04893959313631058\n",
      "epoch :  2171  loss :  0.048898592591285706\n",
      "epoch :  2172  loss :  0.04885590821504593\n",
      "epoch :  2173  loss :  0.048817552626132965\n",
      "epoch :  2174  loss :  0.048773251473903656\n",
      "epoch :  2175  loss :  0.04873606935143471\n",
      "epoch :  2176  loss :  0.04869191721081734\n",
      "epoch :  2177  loss :  0.04865390062332153\n",
      "epoch :  2178  loss :  0.048610687255859375\n",
      "epoch :  2179  loss :  0.04857230558991432\n",
      "epoch :  2180  loss :  0.04852883145213127\n",
      "epoch :  2181  loss :  0.048492953181266785\n",
      "epoch :  2182  loss :  0.04844706878066063\n",
      "epoch :  2183  loss :  0.04841182380914688\n",
      "epoch :  2184  loss :  0.048365846276283264\n",
      "epoch :  2185  loss :  0.04832832142710686\n",
      "epoch :  2186  loss :  0.04828731343150139\n",
      "epoch :  2187  loss :  0.048246465623378754\n",
      "epoch :  2188  loss :  0.048207838088274\n",
      "epoch :  2189  loss :  0.04816454276442528\n",
      "epoch :  2190  loss :  0.04812806472182274\n",
      "epoch :  2191  loss :  0.04808419570326805\n",
      "epoch :  2192  loss :  0.04804695397615433\n",
      "epoch :  2193  loss :  0.048004522919654846\n",
      "epoch :  2194  loss :  0.047967150807380676\n",
      "epoch :  2195  loss :  0.04792468994855881\n",
      "epoch :  2196  loss :  0.04788631200790405\n",
      "epoch :  2197  loss :  0.04784435033798218\n",
      "epoch :  2198  loss :  0.04780679568648338\n",
      "epoch :  2199  loss :  0.04776456952095032\n",
      "epoch :  2200  loss :  0.04772676154971123\n",
      "epoch :  2201  loss :  0.04768548533320427\n",
      "epoch :  2202  loss :  0.04764610156416893\n",
      "epoch :  2203  loss :  0.0476071760058403\n",
      "epoch :  2204  loss :  0.04756522923707962\n",
      "epoch :  2205  loss :  0.047528527677059174\n",
      "epoch :  2206  loss :  0.04748588427901268\n",
      "epoch :  2207  loss :  0.04745019972324371\n",
      "epoch :  2208  loss :  0.047406747937202454\n",
      "epoch :  2209  loss :  0.04737018793821335\n",
      "epoch :  2210  loss :  0.04732876271009445\n",
      "epoch :  2211  loss :  0.047290947288274765\n",
      "epoch :  2212  loss :  0.04724983498454094\n",
      "epoch :  2213  loss :  0.04721226543188095\n",
      "epoch :  2214  loss :  0.04717089980840683\n",
      "epoch :  2215  loss :  0.04713375121355057\n",
      "epoch :  2216  loss :  0.04709269106388092\n",
      "epoch :  2217  loss :  0.04705461859703064\n",
      "epoch :  2218  loss :  0.04701520875096321\n",
      "epoch :  2219  loss :  0.046975184231996536\n",
      "epoch :  2220  loss :  0.04693826660513878\n",
      "epoch :  2221  loss :  0.046897076070308685\n",
      "epoch :  2222  loss :  0.046861909329891205\n",
      "epoch :  2223  loss :  0.046818070113658905\n",
      "epoch :  2224  loss :  0.046783119440078735\n",
      "epoch :  2225  loss :  0.04674128443002701\n",
      "epoch :  2226  loss :  0.04670477658510208\n",
      "epoch :  2227  loss :  0.046663712710142136\n",
      "epoch :  2228  loss :  0.046627383679151535\n",
      "epoch :  2229  loss :  0.04658565670251846\n",
      "epoch :  2230  loss :  0.046550191938877106\n",
      "epoch :  2231  loss :  0.04650896042585373\n",
      "epoch :  2232  loss :  0.04646959528326988\n",
      "epoch :  2233  loss :  0.04643312841653824\n",
      "epoch :  2234  loss :  0.04639143496751785\n",
      "epoch :  2235  loss :  0.046359509229660034\n",
      "epoch :  2236  loss :  0.046314410865306854\n",
      "epoch :  2237  loss :  0.04628249630331993\n",
      "epoch :  2238  loss :  0.046238433569669724\n",
      "epoch :  2239  loss :  0.04620453342795372\n",
      "epoch :  2240  loss :  0.0461629182100296\n",
      "epoch :  2241  loss :  0.04612454026937485\n",
      "epoch :  2242  loss :  0.04608731344342232\n",
      "epoch :  2243  loss :  0.04604838043451309\n",
      "epoch :  2244  loss :  0.046010442078113556\n",
      "epoch :  2245  loss :  0.04597235471010208\n",
      "epoch :  2246  loss :  0.045934244990348816\n",
      "epoch :  2247  loss :  0.04589611291885376\n",
      "epoch :  2248  loss :  0.04585934057831764\n",
      "epoch :  2249  loss :  0.04581855982542038\n",
      "epoch :  2250  loss :  0.04578753560781479\n",
      "epoch :  2251  loss :  0.045742034912109375\n",
      "epoch :  2252  loss :  0.045712850987911224\n",
      "epoch :  2253  loss :  0.04566744714975357\n",
      "epoch :  2254  loss :  0.04563283175230026\n",
      "epoch :  2255  loss :  0.04559340327978134\n",
      "epoch :  2256  loss :  0.04555610567331314\n",
      "epoch :  2257  loss :  0.04551903158426285\n",
      "epoch :  2258  loss :  0.04548046737909317\n",
      "epoch :  2259  loss :  0.04544370621442795\n",
      "epoch :  2260  loss :  0.04540565609931946\n",
      "epoch :  2261  loss :  0.045371051877737045\n",
      "epoch :  2262  loss :  0.04533105716109276\n",
      "epoch :  2263  loss :  0.04529660567641258\n",
      "epoch :  2264  loss :  0.04525541514158249\n",
      "epoch :  2265  loss :  0.045222871005535126\n",
      "epoch :  2266  loss :  0.04518064111471176\n",
      "epoch :  2267  loss :  0.0451485812664032\n",
      "epoch :  2268  loss :  0.045106541365385056\n",
      "epoch :  2269  loss :  0.04507354646921158\n",
      "epoch :  2270  loss :  0.045032452791929245\n",
      "epoch :  2271  loss :  0.044999029487371445\n",
      "epoch :  2272  loss :  0.044958896934986115\n",
      "epoch :  2273  loss :  0.04492282494902611\n",
      "epoch :  2274  loss :  0.04488684982061386\n",
      "epoch :  2275  loss :  0.04484870657324791\n",
      "epoch :  2276  loss :  0.04481356590986252\n",
      "epoch :  2277  loss :  0.04477504640817642\n",
      "epoch :  2278  loss :  0.044740356504917145\n",
      "epoch :  2279  loss :  0.04470105096697807\n",
      "epoch :  2280  loss :  0.04466676712036133\n",
      "epoch :  2281  loss :  0.04462737962603569\n",
      "epoch :  2282  loss :  0.04459340497851372\n",
      "epoch :  2283  loss :  0.04455466940999031\n",
      "epoch :  2284  loss :  0.04451931267976761\n",
      "epoch :  2285  loss :  0.044481776654720306\n",
      "epoch :  2286  loss :  0.04444602131843567\n",
      "epoch :  2287  loss :  0.04440893977880478\n",
      "epoch :  2288  loss :  0.044373489916324615\n",
      "epoch :  2289  loss :  0.044336698949337006\n",
      "epoch :  2290  loss :  0.04430082067847252\n",
      "epoch :  2291  loss :  0.044264089316129684\n",
      "epoch :  2292  loss :  0.044227633625268936\n",
      "epoch :  2293  loss :  0.04419178515672684\n",
      "epoch :  2294  loss :  0.044154733419418335\n",
      "epoch :  2295  loss :  0.04411923512816429\n",
      "epoch :  2296  loss :  0.044082485139369965\n",
      "epoch :  2297  loss :  0.04404659941792488\n",
      "epoch :  2298  loss :  0.04401125758886337\n",
      "epoch :  2299  loss :  0.043973490595817566\n",
      "epoch :  2300  loss :  0.04394064098596573\n",
      "epoch :  2301  loss :  0.043901585042476654\n",
      "epoch :  2302  loss :  0.04386860132217407\n",
      "epoch :  2303  loss :  0.04383036494255066\n",
      "epoch :  2304  loss :  0.043795786798000336\n",
      "epoch :  2305  loss :  0.04375948756933212\n",
      "epoch :  2306  loss :  0.043725110590457916\n",
      "epoch :  2307  loss :  0.04368969798088074\n",
      "epoch :  2308  loss :  0.04365578293800354\n",
      "epoch :  2309  loss :  0.04361952096223831\n",
      "epoch :  2310  loss :  0.04358384758234024\n",
      "epoch :  2311  loss :  0.04355320334434509\n",
      "epoch :  2312  loss :  0.043514758348464966\n",
      "epoch :  2313  loss :  0.043484173715114594\n",
      "epoch :  2314  loss :  0.043444715440273285\n",
      "epoch :  2315  loss :  0.04341564700007439\n",
      "epoch :  2316  loss :  0.043375518172979355\n",
      "epoch :  2317  loss :  0.04334668815135956\n",
      "epoch :  2318  loss :  0.04330717772245407\n",
      "epoch :  2319  loss :  0.043277494609355927\n",
      "epoch :  2320  loss :  0.04323913902044296\n",
      "epoch :  2321  loss :  0.04320497065782547\n",
      "epoch :  2322  loss :  0.04317092150449753\n",
      "epoch :  2323  loss :  0.043136563152074814\n",
      "epoch :  2324  loss :  0.04310479015111923\n",
      "epoch :  2325  loss :  0.04306837543845177\n",
      "epoch :  2326  loss :  0.0430365689098835\n",
      "epoch :  2327  loss :  0.042999543249607086\n",
      "epoch :  2328  loss :  0.04296906292438507\n",
      "epoch :  2329  loss :  0.04293103888630867\n",
      "epoch :  2330  loss :  0.04290132224559784\n",
      "epoch :  2331  loss :  0.0428641214966774\n",
      "epoch :  2332  loss :  0.04283245652914047\n",
      "epoch :  2333  loss :  0.04279685765504837\n",
      "epoch :  2334  loss :  0.04276438429951668\n",
      "epoch :  2335  loss :  0.042729299515485764\n",
      "epoch :  2336  loss :  0.042697370052337646\n",
      "epoch :  2337  loss :  0.042660992592573166\n",
      "epoch :  2338  loss :  0.04263000935316086\n",
      "epoch :  2339  loss :  0.04259367659687996\n",
      "epoch :  2340  loss :  0.04256260767579079\n",
      "epoch :  2341  loss :  0.04252738505601883\n",
      "epoch :  2342  loss :  0.04249411076307297\n",
      "epoch :  2343  loss :  0.04246091470122337\n",
      "epoch :  2344  loss :  0.04242705926299095\n",
      "epoch :  2345  loss :  0.04239550977945328\n",
      "epoch :  2346  loss :  0.042361244559288025\n",
      "epoch :  2347  loss :  0.04232608526945114\n",
      "epoch :  2348  loss :  0.0422951877117157\n",
      "epoch :  2349  loss :  0.042259249836206436\n",
      "epoch :  2350  loss :  0.042228661477565765\n",
      "epoch :  2351  loss :  0.04219295457005501\n",
      "epoch :  2352  loss :  0.04216153919696808\n",
      "epoch :  2353  loss :  0.04212716966867447\n",
      "epoch :  2354  loss :  0.04209557920694351\n",
      "epoch :  2355  loss :  0.042060453444719315\n",
      "epoch :  2356  loss :  0.04202960804104805\n",
      "epoch :  2357  loss :  0.04199463501572609\n",
      "epoch :  2358  loss :  0.04196321219205856\n",
      "epoch :  2359  loss :  0.04192933812737465\n",
      "epoch :  2360  loss :  0.041896481066942215\n",
      "epoch :  2361  loss :  0.041863955557346344\n",
      "epoch :  2362  loss :  0.041830599308013916\n",
      "epoch :  2363  loss :  0.04179840534925461\n",
      "epoch :  2364  loss :  0.04176526516675949\n",
      "epoch :  2365  loss :  0.04173246771097183\n",
      "epoch :  2366  loss :  0.04169949144124985\n",
      "epoch :  2367  loss :  0.04166800528764725\n",
      "epoch :  2368  loss :  0.04163365811109543\n",
      "epoch :  2369  loss :  0.041604653000831604\n",
      "epoch :  2370  loss :  0.04156745225191116\n",
      "epoch :  2371  loss :  0.04153868928551674\n",
      "epoch :  2372  loss :  0.0415031872689724\n",
      "epoch :  2373  loss :  0.04147214815020561\n",
      "epoch :  2374  loss :  0.04143945872783661\n",
      "epoch :  2375  loss :  0.04140691086649895\n",
      "epoch :  2376  loss :  0.04137512296438217\n",
      "epoch :  2377  loss :  0.04134318605065346\n",
      "epoch :  2378  loss :  0.04130951687693596\n",
      "epoch :  2379  loss :  0.041279494762420654\n",
      "epoch :  2380  loss :  0.041245125234127045\n",
      "epoch :  2381  loss :  0.041214726865291595\n",
      "epoch :  2382  loss :  0.0411808080971241\n",
      "epoch :  2383  loss :  0.04115074872970581\n",
      "epoch :  2384  loss :  0.041116949170827866\n",
      "epoch :  2385  loss :  0.04108794778585434\n",
      "epoch :  2386  loss :  0.04105301946401596\n",
      "epoch :  2387  loss :  0.04102533683180809\n",
      "epoch :  2388  loss :  0.04099001735448837\n",
      "epoch :  2389  loss :  0.04095872491598129\n",
      "epoch :  2390  loss :  0.040928177535533905\n",
      "epoch :  2391  loss :  0.040894828736782074\n",
      "epoch :  2392  loss :  0.040866293013095856\n",
      "epoch :  2393  loss :  0.04083249717950821\n",
      "epoch :  2394  loss :  0.04080291837453842\n",
      "epoch :  2395  loss :  0.04077034443616867\n",
      "epoch :  2396  loss :  0.040743209421634674\n",
      "epoch :  2397  loss :  0.04070832580327988\n",
      "epoch :  2398  loss :  0.04068084433674812\n",
      "epoch :  2399  loss :  0.0406452976167202\n",
      "epoch :  2400  loss :  0.04061882197856903\n",
      "epoch :  2401  loss :  0.04058361053466797\n",
      "epoch :  2402  loss :  0.04055580124258995\n",
      "epoch :  2403  loss :  0.04052228108048439\n",
      "epoch :  2404  loss :  0.04049349203705788\n",
      "epoch :  2405  loss :  0.04046065732836723\n",
      "epoch :  2406  loss :  0.040431909263134\n",
      "epoch :  2407  loss :  0.040398139506578445\n",
      "epoch :  2408  loss :  0.04036998003721237\n",
      "epoch :  2409  loss :  0.04033708944916725\n",
      "epoch :  2410  loss :  0.040307365357875824\n",
      "epoch :  2411  loss :  0.04027630761265755\n",
      "epoch :  2412  loss :  0.040245167911052704\n",
      "epoch :  2413  loss :  0.040214601904153824\n",
      "epoch :  2414  loss :  0.040185555815696716\n",
      "epoch :  2415  loss :  0.04015197977423668\n",
      "epoch :  2416  loss :  0.0401252880692482\n",
      "epoch :  2417  loss :  0.04009021073579788\n",
      "epoch :  2418  loss :  0.04006445035338402\n",
      "epoch :  2419  loss :  0.04002929478883743\n",
      "epoch :  2420  loss :  0.03999975323677063\n",
      "epoch :  2421  loss :  0.03997128829360008\n",
      "epoch :  2422  loss :  0.03993909806013107\n",
      "epoch :  2423  loss :  0.03990999236702919\n",
      "epoch :  2424  loss :  0.03987812250852585\n",
      "epoch :  2425  loss :  0.03984973579645157\n",
      "epoch :  2426  loss :  0.03981717303395271\n",
      "epoch :  2427  loss :  0.03978976979851723\n",
      "epoch :  2428  loss :  0.039756543934345245\n",
      "epoch :  2429  loss :  0.03972902148962021\n",
      "epoch :  2430  loss :  0.03969695419073105\n",
      "epoch :  2431  loss :  0.039667293429374695\n",
      "epoch :  2432  loss :  0.03963781148195267\n",
      "epoch :  2433  loss :  0.039606574922800064\n",
      "epoch :  2434  loss :  0.03957761451601982\n",
      "epoch :  2435  loss :  0.03954701125621796\n",
      "epoch :  2436  loss :  0.03951690346002579\n",
      "epoch :  2437  loss :  0.039487287402153015\n",
      "epoch :  2438  loss :  0.03945774585008621\n",
      "epoch :  2439  loss :  0.03942716866731644\n",
      "epoch :  2440  loss :  0.039398036897182465\n",
      "epoch :  2441  loss :  0.0393679216504097\n",
      "epoch :  2442  loss :  0.03933744132518768\n",
      "epoch :  2443  loss :  0.03930874541401863\n",
      "epoch :  2444  loss :  0.0392770916223526\n",
      "epoch :  2445  loss :  0.03924991935491562\n",
      "epoch :  2446  loss :  0.03921804204583168\n",
      "epoch :  2447  loss :  0.03918985277414322\n",
      "epoch :  2448  loss :  0.039159346371889114\n",
      "epoch :  2449  loss :  0.0391305610537529\n",
      "epoch :  2450  loss :  0.039100319147109985\n",
      "epoch :  2451  loss :  0.039071910083293915\n",
      "epoch :  2452  loss :  0.039040736854076385\n",
      "epoch :  2453  loss :  0.03901265189051628\n",
      "epoch :  2454  loss :  0.03898242115974426\n",
      "epoch :  2455  loss :  0.03895246237516403\n",
      "epoch :  2456  loss :  0.038924530148506165\n",
      "epoch :  2457  loss :  0.038894034922122955\n",
      "epoch :  2458  loss :  0.038865428417921066\n",
      "epoch :  2459  loss :  0.038836102932691574\n",
      "epoch :  2460  loss :  0.03880628943443298\n",
      "epoch :  2461  loss :  0.03877735510468483\n",
      "epoch :  2462  loss :  0.03874822333455086\n",
      "epoch :  2463  loss :  0.038719043135643005\n",
      "epoch :  2464  loss :  0.038689933717250824\n",
      "epoch :  2465  loss :  0.03866157680749893\n",
      "epoch :  2466  loss :  0.038630153983831406\n",
      "epoch :  2467  loss :  0.038604579865932465\n",
      "epoch :  2468  loss :  0.03857182711362839\n",
      "epoch :  2469  loss :  0.03854664787650108\n",
      "epoch :  2470  loss :  0.03851514309644699\n",
      "epoch :  2471  loss :  0.03848735988140106\n",
      "epoch :  2472  loss :  0.038457997143268585\n",
      "epoch :  2473  loss :  0.03842957317829132\n",
      "epoch :  2474  loss :  0.03840003162622452\n",
      "epoch :  2475  loss :  0.03837186098098755\n",
      "epoch :  2476  loss :  0.0383424386382103\n",
      "epoch :  2477  loss :  0.038312871009111404\n",
      "epoch :  2478  loss :  0.038286108523607254\n",
      "epoch :  2479  loss :  0.03825563192367554\n",
      "epoch :  2480  loss :  0.03822806105017662\n",
      "epoch :  2481  loss :  0.03820016235113144\n",
      "epoch :  2482  loss :  0.03816848620772362\n",
      "epoch :  2483  loss :  0.03814379870891571\n",
      "epoch :  2484  loss :  0.038111671805381775\n",
      "epoch :  2485  loss :  0.03808562830090523\n",
      "epoch :  2486  loss :  0.03805576637387276\n",
      "epoch :  2487  loss :  0.03802864998579025\n",
      "epoch :  2488  loss :  0.03799869492650032\n",
      "epoch :  2489  loss :  0.037971895188093185\n",
      "epoch :  2490  loss :  0.037941981106996536\n",
      "epoch :  2491  loss :  0.03791411966085434\n",
      "epoch :  2492  loss :  0.03788641840219498\n",
      "epoch :  2493  loss :  0.03785647824406624\n",
      "epoch :  2494  loss :  0.03783028945326805\n",
      "epoch :  2495  loss :  0.037801213562488556\n",
      "epoch :  2496  loss :  0.037772029638290405\n",
      "epoch :  2497  loss :  0.0377456434071064\n",
      "epoch :  2498  loss :  0.03771551698446274\n",
      "epoch :  2499  loss :  0.03768925368785858\n",
      "epoch :  2500  loss :  0.03765944018959999\n"
     ]
    }
   ],
   "source": [
    "for i in range(2500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "\n",
    "    print(\"epoch : \", (i + 1), \" loss : \", loss.item())\n",
    "    loss.backward() # 역전파 적용\n",
    "\n",
    "    optimizer.step() # 모든 파라미터 최신화\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "source": [
    "## torch.nn.module을 사용해서 많은 간단한 레이어를 복잡한 뉴럴 네트워크로 만들 수 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_class(nn.Module):\n",
    "    def __init__(self, n_in, n_h, n_out):\n",
    "        super(custom_class, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_in, n_h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_h, n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nn = custom_class(n_in, n_h, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch :  1  loss :  0.27303898334503174\nepoch :  2  loss :  0.27303898334503174\nepoch :  3  loss :  0.27303898334503174\nepoch :  4  loss :  0.27303898334503174\nepoch :  5  loss :  0.27303898334503174\nepoch :  6  loss :  0.27303898334503174\nepoch :  7  loss :  0.27303898334503174\nepoch :  8  loss :  0.27303898334503174\nepoch :  9  loss :  0.27303898334503174\nepoch :  10  loss :  0.27303898334503174\nepoch :  11  loss :  0.27303898334503174\nepoch :  12  loss :  0.27303898334503174\nepoch :  13  loss :  0.27303898334503174\nepoch :  14  loss :  0.27303898334503174\nepoch :  15  loss :  0.27303898334503174\nepoch :  16  loss :  0.27303898334503174\nepoch :  17  loss :  0.27303898334503174\nepoch :  18  loss :  0.27303898334503174\nepoch :  19  loss :  0.27303898334503174\nepoch :  20  loss :  0.27303898334503174\nepoch :  21  loss :  0.27303898334503174\nepoch :  22  loss :  0.27303898334503174\nepoch :  23  loss :  0.27303898334503174\nepoch :  24  loss :  0.27303898334503174\nepoch :  25  loss :  0.27303898334503174\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    y_pred = custom_nn(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "\n",
    "    print(\"epoch : \", (i + 1), \" loss : \", loss.item())\n",
    "    loss.backward() # 역전파 적용\n",
    "\n",
    "    optimizer.step() # 모든 파라미터 최신화\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.nn : neural network를 만드는데 꼭 필요한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 8., 16., 24.])\n"
     ]
    }
   ],
   "source": [
    "class EgLayer(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.param\n",
    "\n",
    "myOb= EgLayer(8)\n",
    "\n",
    "output = myOb(torch.Tensor([1, 2, 3]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out = 6, 4, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 1.3514,  1.8417,  0.4698, -0.0643,  2.0279,  1.4978]]),\n",
       " tensor([[-1.6934]]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x = torch.randn((1, n_in))\n",
    "y = torch.randn((1, n_out))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-2.0724, -0.4351,  0.8066, -1.9947]]), tensor([[-0.5564]]))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "b1 = torch.randn((1, n_h))\n",
    "b2 = torch.randn((1, n_out))\n",
    "b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-1.7803,  0.2866, -0.5959,  0.0068],\n",
       "         [-1.0222,  0.4391,  0.4681, -1.5267],\n",
       "         [ 0.0290,  1.2527,  0.1160, -0.3396],\n",
       "         [ 0.6189,  0.1403,  2.0113,  0.8271],\n",
       "         [ 0.7983,  0.5444,  0.7498,  0.0356],\n",
       "         [ 1.6316, -0.2317, -0.4515,  0.5038]]),\n",
       " tensor([[-0.0023],\n",
       "         [ 0.7440],\n",
       "         [-0.4143],\n",
       "         [-0.5438]]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "w1 = torch.randn(n_in, n_h)\n",
    "w2 = torch.randn(n_h, n_out)\n",
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.mm(x, w1) + b1\n",
    "a1 = sigmoid_activation(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = torch.mm(a1, w2) + b2\n",
    "output = sigmoid_activation(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_delta(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_out = sigmoid_delta(output)\n",
    "delta_h = sigmoid_delta(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_outp = loss * delta_out\n",
    "loss_h = torch.mm(d_outp, w2.t())\n",
    "d_hidn = loss_h * delta_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}